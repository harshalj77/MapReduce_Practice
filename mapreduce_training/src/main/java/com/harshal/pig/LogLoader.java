package com.harshal.pig;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.pig.FileInputLoadFunc;
import org.apache.pig.PigException;
import org.apache.pig.ResourceSchema;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Parses & Loads log events generated by cloudwick labs generator
 *
 * @author harshal
 */
public class LogLoader extends FileInputLoadFunc {

    private static final Log LOG = LogFactory.getLog(LogLoader.class);
    protected RecordReader in = null;
    protected Pattern httpLogPattern = Pattern.compile("^([\\d.]+) (\\S+) (\\S+) \\[(.*)\\] \"([^\\s]+) (/[^\\s]*) HTTP/[^\\s]+\" (\\d{3}) (\\d+) \"([^\"]+)\" \"([^\"]+)\"$");
    protected Matcher matcher = null;
    private ArrayList<Object> tuple = null;
    private TupleFactory mTupleFactory = TupleFactory.getInstance();

    /*
        Lets the loader know about the location of the input data
     */
    @Override
    public void setLocation(String location, Job job) throws IOException {
        FileInputFormat.setInputPaths(job, location);
    }

    /*
        Retrieves the MapReduce InputFormat that should be used to read the input data
     */
    @Override
    public InputFormat getInputFormat() throws IOException {
        return new TextInputFormat();
    }

    /*
        Called once prior to getNext() being called
     */
    @Override
    public void prepareToRead(RecordReader recordReader, PigSplit pigSplit) throws IOException {
        in = recordReader;
    }

    /*
        Implementers should retrieve the next record from the RecordReader, map it to a Pig Tuple, and return the tuple.
     */
    @Override
    public Tuple getNext() throws IOException {
        Tuple t = null;
        try {
            if (!in.nextKeyValue())
                return null;
            matcher = httpLogPattern.matcher(in.getCurrentValue().toString());
            if (matcher.matches()) {
                // Create the tuple we will be returning. We create it with the right
                // number of fields, as the Tuple object is optimized for this case.
                t = mTupleFactory.newTuple(matcher.groupCount());


                // Read each field in the record
                for (int i = 0; i < matcher.groupCount(); i++)
                    t.set(i, matcher.group(i + 1));
            } else {
                LOG.warn("Bad Record: " + in.getCurrentKey());
            }
        } catch (InterruptedException e) {
            int errCode = 6018;
            String errMsg = "Error while reading input";
            throw new ExecException(errMsg, errCode,PigException.REMOTE_ENVIRONMENT, e);
        }
        return t;
    }

    public ResourceSchema getSchema(String location, Job job) throws IOException {
        return new ResourceSchema(new Schema(
              Arrays.asList(
                    new Schema.FieldSchema("ip", DataType.CHARARRAY),
                    new Schema.FieldSchema("clientIdentity", DataType.CHARARRAY),
                    new Schema.FieldSchema("userId", DataType.CHARARRAY),
                    new Schema.FieldSchema("timeStamp", DataType.CHARARRAY),
                    new Schema.FieldSchema("requestType", DataType.CHARARRAY),
                    new Schema.FieldSchema("requestPage", DataType.LONG),
                    new Schema.FieldSchema("responseCode", DataType.LONG),
                    new Schema.FieldSchema("responseSize", DataType.CHARARRAY),
                    new Schema.FieldSchema("referrer", DataType.CHARARRAY),
                    new Schema.FieldSchema("userAgent", DataType.CHARARRAY)
              )));
    }
}
